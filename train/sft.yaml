# 2*12G
# losses: plugin/loss.py
# data format: docs/source_en/Customization/Custom-dataset.md
# --use_chat_template must be false to use generation template
# --dataloader_drop_last must be true or eval gather will throw error
# --model iic/gte-modernbert-base iic/gte_Qwen2-7B-instruct also supported
# INFONCE_TEMPERATURE default value is 0.01, here we use 0.1 because it makes
# the `sentence-transformers/stsb:positive` dataset result to a zero loss

# Model args
model: /data/k8s/qzq/EmbeddingExplore/embedding_pipeline/models/Qwen3-Embedding-0.6B
model_type: qwen3_emb
task_type: embedding
train_type: full

# Dataset args
dataset: /data/k8s/qzq/EmbeddingExplore/embedding_pipeline/swift_training_data.jsonl:positive
split_dataset_ratio: 0.05
label_names: labels

# Training args
output_dir: output
num_train_epochs: 1
learning_rate: 6e-6
gradient_accumulation_steps: 4
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
dataloader_drop_last: true

# Eval and save args
eval_strategy: steps
eval_steps: 10
save_steps: 10

# Loss and optimization args
loss_type: infonce

# DeepSpeed config
deepspeed: zero3

# Hardware config
# nproc_per_node: 4  # 可以在命令行中指定

# 其他可能需要添加的配置
# gradient_checkpointing: true  # 如果需要可以启用
# fp16: true  # 如果需要混合精度训练
# logging_steps: 10  # 如果需要不同的日志频率